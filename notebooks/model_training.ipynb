{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "571e4937",
   "metadata": {},
   "source": [
    "# FPL Player Points Prediction Model\n",
    "\n",
    "This notebook trains a machine learning model to predict Fantasy Premier League player points for upcoming gameweeks.\n",
    "\n",
    "## Steps:\n",
    "1. Load and explore the processed FPL data\n",
    "2. Feature engineering and selection\n",
    "3. Train multiple regression models\n",
    "4. Evaluate model performance\n",
    "5. Save the best model for use in optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd8aa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "# Utilities\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8283f4d3",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58fe779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed player data\n",
    "data_path = \"../data/processed/fpl_players_latest.csv\"\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"‚úÖ Data loaded successfully! Shape: {df.shape}\")\n",
    "else:\n",
    "    print(\"‚ùå Data file not found. Please run 'python src/fetch_fpl_data.py' first.\")\n",
    "    raise FileNotFoundError(f\"Data file not found at {data_path}\")\n",
    "\n",
    "# Display basic info\n",
    "print(\"\\nDataset Info:\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "print(f\"Rows: {len(df)}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380a398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nDataset statistics:\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81982c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_data = df.isnull().sum()\n",
    "missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_data) > 0:\n",
    "    print(\"Missing values:\")\n",
    "    print(missing_data)\n",
    "else:\n",
    "    print(\"‚úÖ No missing values found!\")\n",
    "\n",
    "# Position distribution\n",
    "print(\"\\nPosition distribution:\")\n",
    "print(df['position'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e7e8b0",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4995d622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations for key variables\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Total points distribution\n",
    "axes[0, 0].hist(df['total_points'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Distribution of Total Points')\n",
    "axes[0, 0].set_xlabel('Total Points')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Cost distribution\n",
    "axes[0, 1].hist(df['cost'], bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[0, 1].set_title('Distribution of Player Cost')\n",
    "axes[0, 1].set_xlabel('Cost (¬£m)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Form vs Total Points\n",
    "axes[1, 0].scatter(df['form'], df['total_points'], alpha=0.6, color='coral')\n",
    "axes[1, 0].set_title('Form vs Total Points')\n",
    "axes[1, 0].set_xlabel('Form')\n",
    "axes[1, 0].set_ylabel('Total Points')\n",
    "\n",
    "# Cost vs Total Points\n",
    "axes[1, 1].scatter(df['cost'], df['total_points'], alpha=0.6, color='gold')\n",
    "axes[1, 1].set_title('Cost vs Total Points')\n",
    "axes[1, 1].set_xlabel('Cost (¬£m)')\n",
    "axes[1, 1].set_ylabel('Total Points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29297db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position-wise analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Box plot of points by position\n",
    "df.boxplot(column='total_points', by='position', ax=axes[0])\n",
    "axes[0].set_title('Total Points by Position')\n",
    "axes[0].set_xlabel('Position')\n",
    "axes[0].set_ylabel('Total Points')\n",
    "\n",
    "# Box plot of cost by position\n",
    "df.boxplot(column='cost', by='position', ax=axes[1])\n",
    "axes[1].set_title('Cost by Position')\n",
    "axes[1].set_xlabel('Position')\n",
    "axes[1].set_ylabel('Cost (¬£m)')\n",
    "\n",
    "plt.suptitle('')  # Remove the automatic title\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7c34c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "# Select numerical columns for correlation\n",
    "numerical_cols = ['total_points', 'form', 'cost', 'minutes', 'points_per_game', \n",
    "                 'influence', 'creativity', 'threat', 'ict_index', 'selected_by_percent',\n",
    "                 'minutes_per_game', 'cost_efficiency']\n",
    "\n",
    "# Filter columns that exist in the dataframe\n",
    "available_cols = [col for col in numerical_cols if col in df.columns]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df[available_cols].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5)\n",
    "plt.title('Correlation Heatmap of Key Variables')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show highest correlations with total_points\n",
    "correlations = correlation_matrix['total_points'].abs().sort_values(ascending=False)\n",
    "print(\"\\nFeatures most correlated with total_points:\")\n",
    "print(correlations[1:10])  # Exclude self-correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1aebad",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86538f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional features\n",
    "df_features = df.copy()\n",
    "\n",
    "# Additional engineered features\n",
    "df_features['points_per_minute'] = df_features['total_points'] / (df_features['minutes'] + 1)  # +1 to avoid division by zero\n",
    "df_features['form_efficiency'] = df_features['form'] / df_features['cost']\n",
    "df_features['ict_per_cost'] = df_features['ict_index'] / df_features['cost']\n",
    "df_features['popularity_ratio'] = df_features['selected_by_percent'] / 100\n",
    "\n",
    "# Goals and assists combined\n",
    "df_features['goals_assists'] = df_features['goals_scored'] + df_features['assists']\n",
    "\n",
    "# Defensive actions (for defenders and goalkeepers)\n",
    "df_features['defensive_actions'] = df_features['clean_sheets'] + df_features['saves']\n",
    "\n",
    "print(\"Additional features created:\")\n",
    "print(\"- points_per_minute\")\n",
    "print(\"- form_efficiency\")\n",
    "print(\"- ict_per_cost\")\n",
    "print(\"- popularity_ratio\")\n",
    "print(\"- goals_assists\")\n",
    "print(\"- defensive_actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f399a8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features for modeling\n",
    "# Target variable\n",
    "target = 'form'  # We'll predict form as a proxy for next gameweek points\n",
    "\n",
    "# Feature selection\n",
    "feature_columns = [\n",
    "    'cost', 'total_points', 'points_per_game', 'minutes', \n",
    "    'influence', 'creativity', 'threat', 'ict_index',\n",
    "    'selected_by_percent', 'minutes_per_game', 'cost_efficiency',\n",
    "    'position_encoded', 'goals_assists', 'defensive_actions',\n",
    "    'points_per_minute', 'form_efficiency', 'ict_per_cost'\n",
    "]\n",
    "\n",
    "# Filter features that exist in the dataframe\n",
    "available_features = [col for col in feature_columns if col in df_features.columns]\n",
    "\n",
    "print(f\"Selected {len(available_features)} features for modeling:\")\n",
    "for i, feature in enumerate(available_features, 1):\n",
    "    print(f\"{i:2d}. {feature}\")\n",
    "\n",
    "# Prepare data for modeling\n",
    "X = df_features[available_features].copy()\n",
    "y = df_features[target].copy()\n",
    "\n",
    "# Handle any remaining missing values\n",
    "X = X.fillna(0)\n",
    "y = y.fillna(0)\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"Missing values in X: {X.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in y: {y.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d2dfa5",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d756f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=df_features['position']\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Scale features for some models\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f6021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to test\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=0.1),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42, eval_metric='rmse')\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Use scaled data for linear models\n",
    "    if 'Regression' in name:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'mae': mae,\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  R¬≤: {r2:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f553856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'MAE': [results[model]['mae'] for model in results.keys()],\n",
    "    'RMSE': [results[model]['rmse'] for model in results.keys()],\n",
    "    'R¬≤': [results[model]['r2'] for model in results.keys()]\n",
    "})\n",
    "\n",
    "comparison_df = comparison_df.sort_values('R¬≤', ascending=False)\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "display(comparison_df)\n",
    "\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "print(f\"\\nüèÜ Best performing model: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcab6219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, (name, result) in enumerate(results.items()):\n",
    "    if i < 5:  # We have 5 models\n",
    "        y_pred = result['predictions']\n",
    "        \n",
    "        # Actual vs Predicted scatter plot\n",
    "        axes[i].scatter(y_test, y_pred, alpha=0.6)\n",
    "        axes[i].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "        axes[i].set_xlabel('Actual')\n",
    "        axes[i].set_ylabel('Predicted')\n",
    "        axes[i].set_title(f'{name}\\nR¬≤ = {result[\"r2\"]:.3f}')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Remove the last subplot (we only have 5 models)\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a39a51c",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3455d487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for tree-based models\n",
    "tree_models = ['Random Forest', 'XGBoost']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for i, model_name in enumerate(tree_models):\n",
    "    if model_name in results:\n",
    "        model = results[model_name]['model']\n",
    "        \n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': available_features,\n",
    "                'importance': model.feature_importances_\n",
    "            }).sort_values('importance', ascending=True)\n",
    "            \n",
    "            # Plot horizontal bar chart\n",
    "            axes[i].barh(importance_df['feature'], importance_df['importance'])\n",
    "            axes[i].set_title(f'{model_name} - Feature Importance')\n",
    "            axes[i].set_xlabel('Importance')\n",
    "            \n",
    "            print(f\"\\nTop 5 features for {model_name}:\")\n",
    "            top_features = importance_df.tail(5)\n",
    "            for _, row in top_features.iterrows():\n",
    "                print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e354a50",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeb875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for the best model\n",
    "if best_model_name == 'XGBoost':\n",
    "    print(\"Tuning XGBoost hyperparameters...\")\n",
    "    \n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.1, 0.2],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    xgb_model = xgb.XGBRegressor(random_state=42, eval_metric='rmse')\n",
    "    grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "elif best_model_name == 'Random Forest':\n",
    "    print(\"Tuning Random Forest hyperparameters...\")\n",
    "    \n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [5, 10, None],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    }\n",
    "    \n",
    "    rf_model = RandomForestRegressor(random_state=42)\n",
    "    grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "else:\n",
    "    # Use the original best model\n",
    "    best_model = results[best_model_name]['model']\n",
    "    print(f\"Using original {best_model_name} model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2039cbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the final tuned model\n",
    "if 'Regression' in best_model_name:\n",
    "    final_predictions = best_model.predict(X_test_scaled)\n",
    "else:\n",
    "    final_predictions = best_model.predict(X_test)\n",
    "\n",
    "final_mae = mean_absolute_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(mean_squared_error(y_test, final_predictions))\n",
    "final_r2 = r2_score(y_test, final_predictions)\n",
    "\n",
    "print(\"\\nFinal Model Performance:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Model: {best_model_name}\")\n",
    "print(f\"MAE: {final_mae:.4f}\")\n",
    "print(f\"RMSE: {final_rmse:.4f}\")\n",
    "print(f\"R¬≤: {final_r2:.4f}\")\n",
    "\n",
    "# Final prediction plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(y_test, final_predictions, alpha=0.6, s=50)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Form')\n",
    "plt.ylabel('Predicted Form')\n",
    "plt.title(f'Final Model: {best_model_name}\\nR¬≤ = {final_r2:.3f}, RMSE = {final_rmse:.3f}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aea2dec",
   "metadata": {},
   "source": [
    "## 7. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b013a9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save the best model\n",
    "model_path = '../models/model.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "print(f\"‚úÖ Model saved to {model_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'features': available_features,\n",
    "    'performance': {\n",
    "        'mae': final_mae,\n",
    "        'rmse': final_rmse,\n",
    "        'r2': final_r2\n",
    "    },\n",
    "    'target': target,\n",
    "    'train_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "metadata_path = '../models/model_metadata.pkl'\n",
    "with open(metadata_path, 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print(f\"‚úÖ Model metadata saved to {metadata_path}\")\n",
    "\n",
    "# Save feature scaler if needed\n",
    "if 'Regression' in best_model_name:\n",
    "    scaler_path = '../models/scaler.pkl'\n",
    "    with open(scaler_path, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    print(f\"‚úÖ Feature scaler saved to {scaler_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd602de6",
   "metadata": {},
   "source": [
    "## 8. Model Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead832c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ Successfully trained and evaluated {len(models)} models\")\n",
    "print(f\"üèÜ Best model: {best_model_name}\")\n",
    "print(f\"üìä Final R¬≤ score: {final_r2:.4f}\")\n",
    "print(f\"üìÅ Model saved to: {model_path}\")\n",
    "print(f\"üéØ Target variable: {target} (as proxy for expected points)\")\n",
    "print(f\"üìà Features used: {len(available_features)}\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. üîß Run the optimizer: python src/optimizer.py\")\n",
    "print(\"2. üåê Launch web app: streamlit run web_app/app.py\")\n",
    "print(\"3. üîÑ Retrain periodically with new gameweek data\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show some example predictions\n",
    "print(\"\\nExample Predictions (Top 10 by predicted form):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create predictions for all data\n",
    "if 'Regression' in best_model_name:\n",
    "    X_scaled = scaler.transform(X)\n",
    "    all_predictions = best_model.predict(X_scaled)\n",
    "else:\n",
    "    all_predictions = best_model.predict(X)\n",
    "\n",
    "df_with_predictions = df_features.copy()\n",
    "df_with_predictions['predicted_form'] = all_predictions\n",
    "\n",
    "top_predictions = df_with_predictions.nlargest(10, 'predicted_form')[['name', 'position', 'team', 'cost', 'form', 'predicted_form']]\n",
    "display(top_predictions)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
